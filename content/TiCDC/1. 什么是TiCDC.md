# 简介

## 什么是TiCDC

[TiCDC](https://github.com/pingcap/ticdc) 是一款通过拉取 TiKV 变更日志实现的 TiDB 增量数据同步工具，具有将数据还原到与上游任意 TSO 一致状态的能力，同时提供[开放数据协议](https://docs.pingcap.com/zh/tidb/stable/ticdc-open-protocol) (TiCDC Open Protocol)，支持其他系统订阅数据变更。（简单来说就是把TiKV中数据的变化实时同步到下游）

## 为什么要用TiCDC

在 TiDB 4.0 之前，TiDB 提供 TiDB Binlog 实现向下游平台的近实时复制，在 TiDB 4.0 中，引入 TiCDC 作为 TiDB 变更数据的捕获框架。 TiCDC 首个 GA 版本随着 TiDB 4.0.6 正式发布，具备生产环境的运行能力，主要优势如下：

- 数据高可用：TiCDC 从 TiKV 获取变更日志，意味着只要 TiKV 具备高可用就能保证数据的高可用，遇到 TiCDC 全部异常关闭的极端情况，后续启动还能正常获取数据。
- 水平扩展：支持组建多 TiCDC 节点集群，将同步任务均匀地调度到不同节点上，面对海量数据时，可以通过添加节点解决同步压力过大的问题。
- 自动故障转移：当集群中的一个 TiCDC 节点意外退出时，该节点上的同步任务会自动调度到其余的 TiCDC 节点上。
- 支持多种下游系统和输出多种格式：目前已经支持兼容 MySQL 协议的数据库、Kafka 和 Pulsar 分布式流处理系统，支持输出的格式有 [Apache Avro](http://avro.apache.org/)，[Maxwell](http://maxwells-daemon.io/) 和 [Canal](https://github.com/alibaba/canal)。

## 使用场景

### [两中心主备](https://docs.google.com/document/d/14iWF-i63kuq-qleLpOwU60ilf0nmpGvoloTZgrcj3V8/edit#heading=h.ru7zgbsl26o1)

数据库作为企业 IT 的核心，在稳定运行的基础之上，数据库的容灾建设成为保障业务连续性的前提条件。

综合考量业务关键性、成本等因素，部分用户希望核心数据库只需要完成主备站点的容灾即可，利用 TiCDC 构建 TiDB 主备站点的容灾方案成为理想之选。该方案基于 TiCDC 的数据同步功能，可以适配两个中心长距离间隔、网络延迟较大的场景，进行两个数据中心 TiDB 集群之间的数据单向同步，保障事务的最终一致性，实现秒级 RPO 。

### [环形同步与多活](https://docs.google.com/document/d/14iWF-i63kuq-qleLpOwU60ilf0nmpGvoloTZgrcj3V8/edit#heading=h.w0q86192ihj9)

利用 TiCDC 实现三个 TiDB 集群之间的环形同步，构建 TiDB 多中心的容灾方案。当一个数据中心的机柜发生意外掉电，可以把业务切换到另一个数据中心的 TiDB 集群，实现事务的最终一致性和秒级 RPO。为了分担业务访问的压力，在应用层可以随时切换路由，将流量切换到目前负载较小的 TiDB 集群提供服务，实现负载均衡，在满足数据高可用的同时提升容灾能力。

![](https://tva1.sinaimg.cn/large/008i3skNly1gri4pnual3j318g0lgn00.jpg)

### [数据订阅](https://docs.google.com/document/d/14iWF-i63kuq-qleLpOwU60ilf0nmpGvoloTZgrcj3V8/edit#heading=h.ufrcxuk29yew)

TiCDC 为下游数据消费端提供实时、高吞吐、稳定的数据订阅服务，通过开放数据协议（Open Protocol ）与 MySQL、Kafka、Pulsar、Flink、Canal、Maxwell 等多种异构生态系统对接，满足用户在大数据场景中对各类数据的应用与分析需求，广泛适用于日志收集、监控数据聚合、流式数据处理、在线和离线分析等场景。

## 架构

TiCDC运行时是一种无状态节点，通过PD内部的etcd来实现高可用。TiCDC集群支持创建多个同步任务，向多个不同的下游进行数据同步。

TiCDC的整体架构如下图所示：

![](https://tva1.sinaimg.cn/large/008i3skNly1gqy53x420lj31xd0u07ek.jpg)

## 数据流转图

![](https://tva1.sinaimg.cn/large/008i3skNly1gre4mob9u9j31160is0wd.jpg)

### 系统角色

- capture：就是一个cdc的实例

- owner：owner 会维护全局的同步状态，会对集群的同步进行监控和适当的调度，owner 运行有以下逻辑：

  - table 同步调度调度 table 的同步任务，分发到某个节点或从某个节点删除
  - 维护运行的 processor 状态信息，对于异常节点进行清理
  - 执行处理 DDL，向下游同步 DDL
  - 更新每一个 changefeed 的全局的 Checkpoint ts 和 Resolved ts 
    - 首先这个TS是事务上的TS
    - CheckpointTS：
      - 代表 cdc 写到下游的事务的 TS，当前 TS 代表数据已经写入成功写入下游
      - 这个 TS 是对于 Changefeed 级别来说的
    - ResolvedTS：
      - 代表TiKV 写到 ticdc 上的数据的 TS，所有小于 resolvedTS 的 commit 的事务已经完整发送到 ticdc 上
      - 这个 ts 是根据该所有的 Region 的 ResolvedTS 计算出来的。
    - Txn: StartTS = 10,  Current TSO = 14, ResolvedTS < 10
      - CommitTS = 15
      - ResolvedTS = 16
      - Prewrite 10 + Commit 15 ⇒ 完整的已经提交的事务

- processor：实际负责将 TiKV 中数据同步到下游的工作线程。当 capture watch etcd 发现有新分配的 changefeed 同步任务到自己节点上时，会创建一个新的 processor 来进行数据同步，它会负责几个table，并将这些表的 table id 按照 TiDB 内部的 key 编码逻辑进行编码得到一些需要获取 kv change log 的 key range，processor 会综合 key range 和 region 分布信息创建多个 EventFeed gRPC stream。具体流程如下

  - 首先每个 table 中的数据是存在一个区间的也就是key range
  - 然后每个 range 中的数据会拆分为多个 region 存在多个 tikv 中
  - cdc 根据要同步的 table 定位到指定的 region 后会去 pd中 拿元信息，也就是对应的 region 存在那个TiKV中，并创建 eventfeed 连接到 TiKV
  - eventfeed 会根据 region id 定位到 TiKV 中对应的 raft group leader 节点上
  - 接下来就是 leader 会将数据变化推送到 TiKV cdc component，eventfeed 同步数据到 cdc 中

  processor具体功能还包括：

  - 维护本地 ResolvedTS 和 CheckpointTS
  - 根据全局 ResolvedTS 推进自己节点的数据向下游同步

## 同步功能介绍

### sink支持

目前 TiCDC sink 模块支持同步数据到以下下游：

- MySQL 协议兼容的数据库，提供最终一致性支持
- 以 TiCDC Open Protocol 输出到 Kafka，可实现行级别有序、最终一致性或严格事务一致性三种一致性保证。

### 同步顺序保证和一致性保证

#### 数据同步顺序

- TiCDC 对于所有的 DDL/DML 都能对外输出**至少一次**。
- TiCDC 在 TiKV/TiCDC 集群故障期间可能会重复发相同的 DDL/DML。对于重复的 DDL/DML：
  - MySQL sink 可以重复执行 DDL，对于在下游可重入的 DDL （譬如 truncate table）直接执行成功；对于在下游不可重入的 DDL（譬如 create table），执行失败，TiCDC 会忽略错误继续同步。
  - Kafka sink 会发送重复的消息，但重复消息不会破坏 ResolvedTS 的约束，用户可以在 Kafka 消费端进行过滤。

#### 数据同步一致性

- MySQL sink
  - TiCDC 不拆分单表事务，**保证**单表事务的原子性。
  - TiCDC **不保证**下游事务的执行顺序和上游完全一致。
  - TiCDC 以表为单位拆分跨表事务，**不保证**跨表事务的原子性。
  - TiCDC **保证**单行的更新与上游更新顺序一致。
- Kafka sink
  - TiCDC 提供不同的数据分发策略，可以按照表、主键或 TS 等策略分发数据到不同 Kafka partition。
  - 不同分发策略下 consumer 的不同实现方式，可以实现不同级别的一致性，包括行级别有序、最终一致性或跨表事务一致性。
  - TiCDC 没有提供 Kafka 消费端实现，只提供了 [TiCDC 开放数据协议](https://docs.pingcap.com/zh/tidb/stable/ticdc-open-protocol)，用户可以依据该协议实现 Kafka 数据的消费端。

## 同步限制

TiCDC 只能同步至少存在一个**有效索引**的表，**有效索引**的定义如下：

- 主键 (`PRIMARY KEY`) 为有效索引。
- 同时满足下列条件的唯一索引  (`UNIQUE INDEX`) 为有效索引：
  - 索引中每一列在表结构中明确定义非空 (`NOT NULL`)。
  - 索引中不存在虚拟生成列 (`VIRTUAL GENERATED COLUMNS`)。

TiCDC 从 4.0.8 版本开始，可通过修改任务配置来同步**没有有效索引**的表，但在数据一致性的保证上有所减弱。具体使用方法和注意事项参考[同步没有有效索引的表](https://docs.pingcap.com/zh/tidb/stable/manage-ticdc#同步没有有效索引的表)。

